# Demo Mode: AI features use canned responses instead of real LLM
# RubyLLM configuration disabled for sanitized demo version
#
# In production, this would connect to an AI service like Ollama or OpenAI
# For this demo, see lib/demo_stubs/ai_service.rb for canned responses

# RubyLLM.configure do |config|
#   config.ollama_api_base = 'http://localhost:11434/v1'
#   config.default_model = 'deepseek-coder-v2:16b'
# end
